{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "agreed-gross",
   "metadata": {},
   "source": [
    "## Code purpose\n",
    "This code allows to recover all the letters on the Newton Project. To do this, it first recover all the letters ID which allows to get the correspondant XML file containing : some metadata about the letter and the text of the letter itself. It then stores these informations in a DataFrame which we save in pickle format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-holocaust",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "homeless-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-advice",
   "metadata": {},
   "source": [
    "## Scraping all article IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-hamburg",
   "metadata": {},
   "source": [
    "On the Newton's Project website, there are 25 articles per page and there are 18 pages of correspondance. We need these informations for the parsing, because of the GET parameters used in the URL to display the letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "after-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_pages = 18\n",
    "articles_per_page = 25\n",
    "total_letters = 431"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-mechanics",
   "metadata": {},
   "source": [
    "For each page, we get its full HTML content and parse it with BeautifulSoup. Then we simply temporarly store it in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "demanding-builder",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "html_soups = []\n",
    "for page in range(number_of_pages):\n",
    "    start_article_number = page * articles_per_page + 1\n",
    "    url = f\"http://www.newtonproject.ox.ac.uk/texts/correspondence/all?n=25&sr={start_article_number}&cat=Correspondence\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    html_soups.append(soup)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-luxury",
   "metadata": {},
   "source": [
    "Using BeautifulSoup, we extract the article IDs from each HTML pages which will allow us to get the correspondings XML. These IDs are all in `p` tags with the `metadataContent` class. As this `p` tag also contains a `strong` tag containing the front-end title \"Newton Catalogue ID\", we need to exclude this tag. All letters ID are composed by 4 upper characters and 5 digits. To retrieve these IDs only, we use a Regular Expression that correspond to the ID characters composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "amended-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_ids = []\n",
    "for soup in html_soups:\n",
    "    metadata = soup.find_all(\"p\", class_=\"metadataContent\")\n",
    "    for metadatum in metadata:\n",
    "        ids = re.findall(\"[A-Z]{4}[0-9]{5}\", metadatum.text)\n",
    "        if len(ids) > 0:\n",
    "            article_ids.append(ids[0])\n",
    "assert(len(article_ids) == total_letters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-laptop",
   "metadata": {},
   "source": [
    "## Retrieve XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-sugar",
   "metadata": {},
   "source": [
    "Once we have a list of IDs, we can simply query the corresponding XML file and parse it with BS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "religious-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_soups = []\n",
    "for article_id in article_ids:\n",
    "    url = f\"http://www.newtonproject.ox.ac.uk/view/texts/xml/{article_id}\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"xml\")\n",
    "    xml_soups.append(soup)\n",
    "\n",
    "assert(len(xml_soups) == total_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-phoenix",
   "metadata": {},
   "source": [
    "We can now simply iterate through each XML and get the useful information. This information is stored in a list of lists that will be used to build a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "golden-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = []\n",
    "for xml in xml_soups:\n",
    "    #Removes abbreviations. e.g. there are 2 versions for the word \"Sir\", namely the abbreviation \"Sr\" and the complete word \"Sir\".\n",
    "    for abbr in xml(\"abbr\"):\n",
    "        abbr.decompose()\n",
    "        \n",
    "    author = xml.find(\"author\").text.replace(\"\\n\", \" \").strip(\" \")\n",
    "    \n",
    "    #This may not be entirely correct to query div\n",
    "    letter_content = xml.find(\"div\").text\n",
    "    if letter_content is not None:\n",
    "        letter_content = \" \".join(letter_content.split())\n",
    "    else:\n",
    "        letter_content = \"\"\n",
    "   \n",
    "    original_date = xml.find(\"origDate\").text\n",
    "    original_place = xml.find(\"origPlace\")\n",
    "    if original_place is not None:\n",
    "        original_place = original_place.text\n",
    "    else:\n",
    "        original_place = \"Unknown\"\n",
    "        \n",
    "    languages = [lang.text for lang in xml.find_all(\"language\")]\n",
    "    \n",
    "    entries.append([author, original_date, original_place, languages, letter_content])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-scottish",
   "metadata": {},
   "source": [
    "Now we can actively build the DataFrame using the article IDs as indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dependent-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = pd.DataFrame(entries, columns=[\"author\", \"original_date\", \"original_place\", \"languages\", \"letter_content\"], index = article_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-joint",
   "metadata": {},
   "source": [
    "Saving DataFrame in pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "floral-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters.to_pickle(\"letters.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
